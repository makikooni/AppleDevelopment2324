{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Pokemon GAN\n",
        "\n",
        "* **Author:** Conor Lazarou\n",
        "* **License:** MIT (for the model, not for the dataset)\n",
        "\n",
        "Make sure you select GPU under Runtime type\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Model from [Github](https://github.com/ConorLazarou/PokeGAN)\n",
        "\n",
        "adapted to google colab by xiaowan\n",
        "\n",
        "Discussion article on [Medium](https://conor-lazarou.medium.com/i-generated-thousands-of-new-pokemon-using-ai-f8f09dc6477e)\n",
        "\n",
        "Dataset from [PokeAPI](https://github.com/PokeAPI/sprites)\n"
      ],
      "metadata": {
        "id": "j_pQH0bpRkmH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare dataset\n",
        "\n",
        "## (download and unzip)\n",
        "\n",
        "this code block only need to run once, check your folder contents on the side bar"
      ],
      "metadata": {
        "id": "DH-H4Zy1RqdI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogFsL_HXLqs3",
        "outputId": "6fbd590c-b147-4b82-938e-5064cbe769fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-15 22:09:10--  https://github.com/PokeAPI/sprites/archive/refs/heads/master.zip\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/PokeAPI/sprites/zip/refs/heads/master [following]\n",
            "--2023-03-15 22:09:10--  https://codeload.github.com/PokeAPI/sprites/zip/refs/heads/master\n",
            "Resolving codeload.github.com (codeload.github.com)... 192.30.255.120\n",
            "Connecting to codeload.github.com (codeload.github.com)|192.30.255.120|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘/tmp/pokemons.zip’\n",
            "\n",
            "/tmp/pokemons.zip       [           <=>      ] 775.41M  11.9MB/s    in 53s     \n",
            "\n",
            "2023-03-15 22:10:04 (14.5 MB/s) - ‘/tmp/pokemons.zip’ saved [813076521]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    \"https://github.com/PokeAPI/sprites/archive/refs/heads/master.zip\" \\\n",
        "    -O \"/tmp/pokemons.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6liBQFcmMWv8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f0a4b9b-43f5-4d77-9f48-c304be51e24f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 0: syntax error near unexpected token `'/content/pokemons''\n",
            "/bin/bash: -c: line 0: `os.makedir('/content/pokemons')'\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "zip_ref = zipfile.ZipFile('/tmp/pokemons.zip', 'r') #Opens the zip file in read mode\n",
        "!os.makedir('/content/pokemons')\n",
        "zip_ref.extractall('/content/pokemons') #Extracts the files into the /tmp folder\n",
        "zip_ref.close()\n",
        "#very bad practice here\n",
        "\n",
        "!rm -r /content/pokemons/sprites-master/sprites/pokemon/back\n",
        "!rm -r /content/pokemons/sprites-master/sprites/pokemon/female\n",
        "!rm -r /content/pokemons/sprites-master/sprites/pokemon/other\n",
        "!rm -r /content/pokemons/sprites-master/sprites/pokemon/shiny\n",
        "!rm -r /content/pokemons/sprites-master/sprites/pokemon/versions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "our dataset is now saved in /content/pokemons"
      ],
      "metadata": {
        "id": "vxMyvtc-R1gX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WT3YCdENLsx"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Voc95vkDMzsf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torchvision as tv\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "EPS = 1e-6\n",
        "ALPHA_RECONSTRUCT_IMAGE = 1\n",
        "ALPHA_RECONSTRUCT_LATENT = 0.5\n",
        "ALPHA_DISCRIMINATE_IMAGE = 0.005\n",
        "ALPHA_DISCRIMINATE_LATENT = 0.1\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"A generator for mapping a latent space to a sample space.\n",
        "\n",
        "    Input shape: (?, latent_dim)\n",
        "    Output shape: (?, 3, 96, 96)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, latent_dim: int = 16):\n",
        "        \"\"\"Initialize generator.\n",
        "\n",
        "        Args:\n",
        "            latent_dim (int): latent dimension (\"noise vector\")\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self._init_modules()\n",
        "\n",
        "    def build_colourspace(self, input_dim: int, output_dim: int):\n",
        "        \"\"\"Build a small module for selecting colours.\"\"\"\n",
        "        colourspace = nn.Sequential(\n",
        "            nn.Linear(\n",
        "                input_dim,\n",
        "                128,\n",
        "                bias=True),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(),\n",
        "\n",
        "            nn.Linear(\n",
        "                128,\n",
        "                64,\n",
        "                bias=True),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.LeakyReLU(),\n",
        "\n",
        "            nn.Linear(\n",
        "                64,\n",
        "                output_dim,\n",
        "                bias=True),\n",
        "            nn.Tanh(),\n",
        "            )\n",
        "        return colourspace\n",
        "\n",
        "    def _init_modules(self):\n",
        "        \"\"\"Initialize the modules.\"\"\"\n",
        "        projection_widths = [8, 8, 8, 8, 8, 8, 8]\n",
        "        self.projection_dim = sum(projection_widths) + self.latent_dim\n",
        "        self.projection = nn.ModuleList()\n",
        "        for index, i in enumerate(projection_widths):\n",
        "            self.projection.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(\n",
        "                        self.latent_dim + sum(projection_widths[:index]),\n",
        "                        i,\n",
        "                        bias=True,\n",
        "                        ),\n",
        "                    nn.BatchNorm1d(8),\n",
        "                    nn.LeakyReLU(),\n",
        "                    )\n",
        "                )\n",
        "        self.projection_upscaler = nn.Upsample(scale_factor=3)\n",
        "\n",
        "        self.colourspace_r = self.build_colourspace(self.projection_dim, 16)\n",
        "        self.colourspace_g = self.build_colourspace(self.projection_dim, 16)\n",
        "        self.colourspace_b = self.build_colourspace(self.projection_dim, 16)\n",
        "        self.colourspace_upscaler = nn.Upsample(scale_factor=96)\n",
        "\n",
        "        self.seed = nn.Sequential(\n",
        "            nn.Linear(\n",
        "                self.projection_dim,\n",
        "                512*3*3,\n",
        "                bias=True),\n",
        "            nn.BatchNorm1d(512*3*3),\n",
        "            nn.LeakyReLU(),\n",
        "            )\n",
        "\n",
        "        self.upscaling = nn.ModuleList()\n",
        "        self.conv = nn.ModuleList()\n",
        "\n",
        "        self.upscaling.append(nn.Upsample(scale_factor=2))\n",
        "        self.conv.append(nn.Sequential(\n",
        "            nn.ZeroPad2d((1, 1, 1, 1)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=(512)//4,\n",
        "                out_channels=512,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=0,\n",
        "                bias=True\n",
        "                ),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(),\n",
        "            ))\n",
        "\n",
        "        self.upscaling.append(nn.Upsample(scale_factor=2))\n",
        "        self.conv.append(nn.Sequential(\n",
        "            nn.ZeroPad2d((1, 2, 1, 2)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=(512 + self.projection_dim)//4,\n",
        "                out_channels=256,\n",
        "                kernel_size=4,\n",
        "                stride=1,\n",
        "                padding=0,\n",
        "                bias=True\n",
        "                ),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(),\n",
        "            ))\n",
        "\n",
        "        self.upscaling.append(nn.Upsample(scale_factor=2))\n",
        "        self.conv.append(nn.Sequential(\n",
        "            nn.ZeroPad2d((1, 2, 1, 2)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=(256 + self.projection_dim)//4,\n",
        "                out_channels=256,\n",
        "                kernel_size=4,\n",
        "                stride=1,\n",
        "                padding=0,\n",
        "                bias=True\n",
        "                ),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(),\n",
        "            ))\n",
        "\n",
        "        self.upscaling.append(nn.Upsample(scale_factor=2))\n",
        "        self.conv.append(nn.Sequential(\n",
        "            nn.ZeroPad2d((1, 2, 1, 2)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=(256 + self.projection_dim)//4,\n",
        "                out_channels=256,\n",
        "                kernel_size=4,\n",
        "                stride=1,\n",
        "                padding=0,\n",
        "                bias=True\n",
        "                ),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(),\n",
        "            )),\n",
        "\n",
        "        self.upscaling.append(nn.Upsample(scale_factor=2))\n",
        "        self.conv.append(nn.Sequential(\n",
        "            nn.ZeroPad2d((1, 2, 1, 2)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=(256 + self.projection_dim)//4,\n",
        "                out_channels=64,\n",
        "                kernel_size=4,\n",
        "                stride=1,\n",
        "                padding=0,\n",
        "                bias=True\n",
        "                ),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(),\n",
        "            ))\n",
        "\n",
        "        self.upscaling.append(nn.Upsample(scale_factor=1))\n",
        "        self.conv.append(nn.Sequential(\n",
        "            nn.ZeroPad2d((2, 2, 2, 2)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=64,\n",
        "                out_channels=16,\n",
        "                kernel_size=5,\n",
        "                stride=1,\n",
        "                padding=0,\n",
        "                bias=True\n",
        "                ),\n",
        "            nn.Softmax(dim=1),\n",
        "            ))\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        \"\"\"Forward pass; map latent vectors to samples.\"\"\"\n",
        "        last = input_tensor\n",
        "        for module in self.projection:\n",
        "            projection = module(last)\n",
        "            last = torch.cat((last, projection), -1)\n",
        "        projection = last\n",
        "\n",
        "        intermediate = self.seed(projection)\n",
        "        intermediate = intermediate.view((-1, 512, 3, 3))\n",
        "\n",
        "        projection_2d = projection.view((-1, self.projection_dim, 1, 1))\n",
        "        projection_2d = self.projection_upscaler(projection_2d)\n",
        "\n",
        "        for i, (conv, upscaling) in enumerate(zip(self.conv, self.upscaling)):\n",
        "            if i + 1 != len(self.upscaling):\n",
        "                if i > 0:\n",
        "                    intermediate = torch.cat((intermediate, projection_2d), 1)\n",
        "                intermediate = torch.nn.functional.pixel_shuffle(intermediate, 2)\n",
        "            intermediate = conv(intermediate)\n",
        "            projection_2d = upscaling(projection_2d)\n",
        "\n",
        "        r_space = self.colourspace_r(projection)\n",
        "        r_space = r_space.view((-1, 16, 1, 1))\n",
        "        r_space = self.colourspace_upscaler(r_space)\n",
        "        r_space = intermediate * r_space\n",
        "        r_space = torch.sum(r_space, dim=1, keepdim=True)\n",
        "\n",
        "        g_space = self.colourspace_g(projection)\n",
        "        g_space = g_space.view((-1, 16, 1, 1))\n",
        "        g_space = self.colourspace_upscaler(g_space)\n",
        "        g_space = intermediate * g_space\n",
        "        g_space = torch.sum(g_space, dim=1, keepdim=True)\n",
        "\n",
        "        b_space = self.colourspace_b(projection)\n",
        "        b_space = b_space.view((-1, 16, 1, 1))\n",
        "        b_space = self.colourspace_upscaler(b_space)\n",
        "        b_space = intermediate * b_space\n",
        "        b_space = torch.sum(b_space, dim=1, keepdim=True)\n",
        "\n",
        "        output = torch.cat((r_space, g_space, b_space), dim=1)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"An Encoder for encoding images as latent vectors.\n",
        "\n",
        "    Input shape: (?, 3, 96, 96)\n",
        "    Output shape: (?, latent_dim)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device: str = \"cpu\", latent_dim: int = 8):\n",
        "        \"\"\"Initialize encoder.\n",
        "\n",
        "        Args:\n",
        "            device: chich GPU or CPU to use.\n",
        "            latent_dim: output dimension\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.latent_dim = latent_dim\n",
        "        self._init_modules()\n",
        "\n",
        "    def _init_modules(self):\n",
        "        \"\"\"Initialize the modules.\"\"\"\n",
        "        down_channels = [3, 64, 128, 256, 512]\n",
        "        self.down = nn.ModuleList()\n",
        "        for i in range(len(down_channels)-1):\n",
        "            self.down.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(\n",
        "                        in_channels=down_channels[i],\n",
        "                        out_channels=down_channels[i+1],\n",
        "                        kernel_size=3,\n",
        "                        stride=2,\n",
        "                        padding=1,\n",
        "                        bias=True,\n",
        "                        ),\n",
        "                    nn.BatchNorm2d(down_channels[i+1]),\n",
        "                    nn.LeakyReLU(),\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        self.reducer = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=down_channels[-1],\n",
        "                out_channels=down_channels[-2],\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=1,\n",
        "                bias=True,\n",
        "                ),\n",
        "            nn.BatchNorm2d(down_channels[-2]),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Upsample(scale_factor=2)\n",
        "            )\n",
        "\n",
        "        up_channels = [256, 128, 64, 64, 64]\n",
        "        scale_factors = [2, 2, 2, 1]\n",
        "        self.up = nn.ModuleList()\n",
        "        for i in range(len(up_channels)-1):\n",
        "            self.up.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(\n",
        "                        in_channels=up_channels[i] + down_channels[-2-i],\n",
        "                        out_channels=up_channels[i+1],\n",
        "                        kernel_size=3,\n",
        "                        stride=1,\n",
        "                        padding=1,\n",
        "                        bias=True,\n",
        "                        ),\n",
        "                    nn.BatchNorm2d(up_channels[i+1]),\n",
        "                    nn.LeakyReLU(),\n",
        "                    nn.Upsample(scale_factor=scale_factors[i]),\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        down_again_channels = [64+3, 64, 64, 64, 64]\n",
        "        self.down_again = nn.ModuleList()\n",
        "        for i in range(len(down_again_channels)-1):\n",
        "            self.down_again.append(\n",
        "                nn.Conv2d(\n",
        "                    in_channels=down_again_channels[i],\n",
        "                    out_channels=down_again_channels[i+1],\n",
        "                    kernel_size=3,\n",
        "                    stride=2,\n",
        "                    padding=1,\n",
        "                    bias=True,\n",
        "                    )\n",
        "                )\n",
        "            self.down_again.append(nn.BatchNorm2d(down_again_channels[i+1]))\n",
        "            self.down_again.append(nn.LeakyReLU())\n",
        "\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(\n",
        "                512*6*6 + 64*6*6,\n",
        "                256,\n",
        "                bias=True,\n",
        "                ),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(),\n",
        "\n",
        "            nn.Linear(\n",
        "                256,\n",
        "                128,\n",
        "                bias=True,\n",
        "                ),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(),\n",
        "\n",
        "            nn.Linear(\n",
        "                128,\n",
        "                self.latent_dim,\n",
        "                bias=True,\n",
        "                ),\n",
        "            )\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        \"\"\"Forward pass; map latent vectors to samples.\"\"\"\n",
        "        rv = torch.randn(input_tensor.size(), device=self.device) * 0.02\n",
        "        augmented_input = input_tensor + rv\n",
        "        intermediate = augmented_input\n",
        "        intermediates = [augmented_input]\n",
        "        for module in self.down:\n",
        "            intermediate = module(intermediate)\n",
        "            intermediates.append(intermediate)\n",
        "        intermediates = intermediates[:-1][::-1]\n",
        "\n",
        "        down = intermediate.view(-1, 6*6*512)\n",
        "\n",
        "        intermediate = self.reducer(intermediate)\n",
        "\n",
        "        for index, module in enumerate(self.up):\n",
        "            intermediate = torch.cat((intermediate, intermediates[index]), 1)\n",
        "            intermediate = module(intermediate)\n",
        "\n",
        "        intermediate = torch.cat((intermediate, input_tensor), 1)\n",
        "\n",
        "        for module in self.down_again:\n",
        "            intermediate = module(intermediate)\n",
        "\n",
        "        intermediate = intermediate.view(-1, 6*6*64)\n",
        "        intermediate = torch.cat((down, intermediate), -1)\n",
        "\n",
        "        projected = self.projection(intermediate)\n",
        "\n",
        "        return projected\n",
        "\n",
        "\n",
        "class DiscriminatorImage(nn.Module):\n",
        "    \"\"\"A discriminator for discerning real from generated images.\n",
        "\n",
        "    Input shape: (?, 3, 96, 96)\n",
        "    Output shape: (?, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device=\"cpu\"):\n",
        "        \"\"\"Initialize the discriminator.\"\"\"\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self._init_modules()\n",
        "\n",
        "    def _init_modules(self):\n",
        "        \"\"\"Initialize the modules.\"\"\"\n",
        "        down_channels = [3, 64, 128, 256, 512]\n",
        "        self.down = nn.ModuleList()\n",
        "        leaky_relu = nn.LeakyReLU()\n",
        "        for i in range(4):\n",
        "            self.down.append(\n",
        "                nn.Conv2d(\n",
        "                    in_channels=down_channels[i],\n",
        "                    out_channels=down_channels[i+1],\n",
        "                    kernel_size=3,\n",
        "                    stride=2,\n",
        "                    padding=1,\n",
        "                    bias=True,\n",
        "                    )\n",
        "                )\n",
        "            self.down.append(nn.BatchNorm2d(down_channels[i+1]))\n",
        "            self.down.append(leaky_relu)\n",
        "\n",
        "        self.classifier = nn.ModuleList()\n",
        "        self.width = down_channels[-1] * 6**2\n",
        "        self.classifier.append(nn.Linear(self.width, 1))\n",
        "        self.classifier.append(nn.Sigmoid())\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        \"\"\"Forward pass; map latent vectors to samples.\"\"\"\n",
        "        rv = torch.randn(input_tensor.size(), device=self.device) * 0.02\n",
        "        intermediate = input_tensor + rv\n",
        "        for module in self.down:\n",
        "            intermediate = module(intermediate)\n",
        "            rv = torch.randn(intermediate.size(), device=self.device) * 0.02 + 1\n",
        "            intermediate *= rv\n",
        "\n",
        "        intermediate = intermediate.view(-1, self.width)\n",
        "\n",
        "        for module in self.classifier:\n",
        "            intermediate = module(intermediate)\n",
        "\n",
        "        return intermediate\n",
        "\n",
        "\n",
        "class DiscriminatorLatent(nn.Module):\n",
        "    \"\"\"A discriminator for discerning real from generated vectors.\n",
        "\n",
        "    Input shape: (?, latent_dim)\n",
        "    Output shape: (?, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, latent_dim=8, device=\"cpu\"):\n",
        "        \"\"\"Initialize the Discriminator.\"\"\"\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.device = device\n",
        "        self._init_modules()\n",
        "\n",
        "    def _init_modules(self, depth=7, width=8):\n",
        "        \"\"\"Initialize the modules.\"\"\"\n",
        "        self.pyramid = nn.ModuleList()\n",
        "        for i in range(depth):\n",
        "            self.pyramid.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(\n",
        "                        self.latent_dim + width*i,\n",
        "                        width,\n",
        "                        bias=True,\n",
        "                        ),\n",
        "                    nn.BatchNorm1d(width),\n",
        "                    nn.LeakyReLU(),\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        self.classifier = nn.ModuleList()\n",
        "        self.classifier.append(nn.Linear(depth*width + self.latent_dim, 1))\n",
        "        self.classifier.append(nn.Sigmoid())\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        \"\"\"Forward pass; map latent vectors to samples.\"\"\"\n",
        "        last = input_tensor\n",
        "        for module in self.pyramid:\n",
        "            projection = module(last)\n",
        "            rv = torch.randn(projection.size(), device=self.device) * 0.02 + 1\n",
        "            projection *= rv\n",
        "            last = torch.cat((last, projection), -1)\n",
        "        for module in self.classifier:\n",
        "            last = module(last)\n",
        "        return last\n",
        "\n",
        "\n",
        "class AEGAN():\n",
        "    \"\"\"An Autoencoder Generative Adversarial Network for making pokemon.\"\"\"\n",
        "\n",
        "    def __init__(self, latent_dim, noise_fn, dataloader,\n",
        "                 batch_size=32, device='cpu'):\n",
        "        \"\"\"Initialize the AEGAN.\n",
        "\n",
        "        Args:\n",
        "            latent_dim: latent-space dimension. Must be divisible by 4.\n",
        "            noise_fn: function f(num: int) -> pytorch tensor, (latent vectors)\n",
        "            dataloader: a pytorch dataloader for loading images\n",
        "            batch_size: training batch size. Must match that of dataloader\n",
        "            device: cpu or CUDA\n",
        "        \"\"\"\n",
        "        assert latent_dim % 4 == 0\n",
        "        self.latent_dim = latent_dim\n",
        "        self.device = device\n",
        "        self.noise_fn = noise_fn\n",
        "        self.dataloader = dataloader\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.criterion_gen = nn.BCELoss()\n",
        "        self.criterion_recon_image = nn.L1Loss()\n",
        "        self.criterion_recon_latent = nn.MSELoss()\n",
        "        self.target_ones = torch.ones((batch_size, 1), device=device)\n",
        "        self.target_zeros = torch.zeros((batch_size, 1), device=device)\n",
        "        self._init_generator()\n",
        "        self._init_encoder()\n",
        "        self._init_dx()\n",
        "        self._init_dz()\n",
        "\n",
        "    def _init_generator(self):\n",
        "        self.generator = Generator(latent_dim=self.latent_dim)\n",
        "        self.generator = self.generator.to(self.device)\n",
        "        self.optim_g = optim.Adam(self.generator.parameters(),\n",
        "                                  lr=2e-4, betas=(0.5, 0.999),\n",
        "                                  weight_decay=1e-8)\n",
        "\n",
        "    def _init_encoder(self):\n",
        "        self.encoder = Encoder(latent_dim=self.latent_dim, device=self.device)\n",
        "        self.encoder = self.encoder.to(self.device)\n",
        "        self.optim_e = optim.Adam(self.encoder.parameters(),\n",
        "                                  lr=2e-4, betas=(0.5, 0.999),\n",
        "                                  weight_decay=1e-8)\n",
        "\n",
        "    def _init_dx(self):\n",
        "        self.discriminator_image = DiscriminatorImage(device=self.device).to(self.device)\n",
        "        self.optim_di = optim.Adam(self.discriminator_image.parameters(),\n",
        "                                   lr=1e-4, betas=(0.5, 0.999),\n",
        "                                   weight_decay=1e-8)\n",
        "\n",
        "    def _init_dz(self):\n",
        "        self.discriminator_latent = DiscriminatorLatent(\n",
        "            latent_dim=self.latent_dim,\n",
        "            device=self.device,\n",
        "            ).to(self.device)\n",
        "        self.optim_dl = optim.Adam(self.discriminator_latent.parameters(),\n",
        "                                   lr=1e-4, betas=(0.5, 0.999),\n",
        "                                   weight_decay=1e-8)\n",
        "\n",
        "\n",
        "    def generate_samples(self, latent_vec=None, num=None):\n",
        "        \"\"\"Sample images from the generator.\n",
        "\n",
        "        Images are returned as a 4D tensor of values between -1 and 1.\n",
        "        Dimensions are (number, channels, height, width). Returns the tensor\n",
        "        on cpu.\n",
        "\n",
        "        Args:\n",
        "            latent_vec: A pytorch latent vector or None\n",
        "            num: The number of samples to generate if latent_vec is None\n",
        "\n",
        "        If latent_vec and num are None then use self.batch_size\n",
        "        random latent vectors.\n",
        "        \"\"\"\n",
        "        num = self.batch_size if num is None else num\n",
        "        latent_vec = self.noise_fn(num) if latent_vec is None else latent_vec\n",
        "        with torch.no_grad():\n",
        "            samples = self.generator(latent_vec)\n",
        "        samples = samples.cpu()  # move images to cpu\n",
        "        return samples\n",
        "\n",
        "    def train_step_generators(self, X):\n",
        "        \"\"\"Train the generator one step and return the loss.\"\"\"\n",
        "        self.generator.zero_grad()\n",
        "        self.encoder.zero_grad()\n",
        "\n",
        "        Z = self.noise_fn(self.batch_size)\n",
        "\n",
        "        X_hat = self.generator(Z)\n",
        "        Z_hat = self.encoder(X)\n",
        "        X_tilde = self.generator(Z_hat)\n",
        "        Z_tilde = self.encoder(X_hat)\n",
        "\n",
        "        X_hat_confidence = self.discriminator_image(X_hat)\n",
        "        Z_hat_confidence = self.discriminator_latent(Z_hat)\n",
        "        X_tilde_confidence = self.discriminator_image(X_tilde)\n",
        "        Z_tilde_confidence = self.discriminator_latent(Z_tilde)\n",
        "\n",
        "        X_hat_loss = self.criterion_gen(X_hat_confidence, self.target_ones)\n",
        "        Z_hat_loss = self.criterion_gen(Z_hat_confidence, self.target_ones)\n",
        "        X_tilde_loss = self.criterion_gen(X_tilde_confidence, self.target_ones)\n",
        "        Z_tilde_loss = self.criterion_gen(Z_tilde_confidence, self.target_ones)\n",
        "\n",
        "        X_recon_loss = self.criterion_recon_image(X_tilde, X) * ALPHA_RECONSTRUCT_IMAGE\n",
        "        Z_recon_loss = self.criterion_recon_latent(Z_tilde, Z) * ALPHA_RECONSTRUCT_LATENT\n",
        "\n",
        "        X_loss = (X_hat_loss + X_tilde_loss) / 2 * ALPHA_DISCRIMINATE_IMAGE\n",
        "        Z_loss = (Z_hat_loss + Z_tilde_loss) / 2 * ALPHA_DISCRIMINATE_LATENT\n",
        "        loss = X_loss + Z_loss + X_recon_loss + Z_recon_loss\n",
        "\n",
        "        loss.backward()\n",
        "        self.optim_e.step()\n",
        "        self.optim_g.step()\n",
        "\n",
        "        return X_loss.item(), Z_loss.item(), X_recon_loss.item(), Z_recon_loss.item()\n",
        "\n",
        "    def train_step_discriminators(self, X):\n",
        "        \"\"\"Train the discriminator one step and return the losses.\"\"\"\n",
        "        self.discriminator_image.zero_grad()\n",
        "        self.discriminator_latent.zero_grad()\n",
        "\n",
        "        Z = self.noise_fn(self.batch_size)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            X_hat = self.generator(Z)\n",
        "            Z_hat = self.encoder(X)\n",
        "            X_tilde = self.generator(Z_hat)\n",
        "            Z_tilde = self.encoder(X_hat)\n",
        "\n",
        "        X_confidence = self.discriminator_image(X)\n",
        "        X_hat_confidence = self.discriminator_image(X_hat)\n",
        "        X_tilde_confidence = self.discriminator_image(X_tilde)\n",
        "        Z_confidence = self.discriminator_latent(Z)\n",
        "        Z_hat_confidence = self.discriminator_latent(Z_hat)\n",
        "        Z_tilde_confidence = self.discriminator_latent(Z_tilde)\n",
        "\n",
        "        X_loss = 2 * self.criterion_gen(X_confidence, self.target_ones)\n",
        "        X_hat_loss = self.criterion_gen(X_hat_confidence, self.target_zeros)\n",
        "        X_tilde_loss = self.criterion_gen(X_tilde_confidence, self.target_zeros)\n",
        "        Z_loss = 2 * self.criterion_gen(Z_confidence, self.target_ones)\n",
        "        Z_hat_loss = self.criterion_gen(Z_hat_confidence, self.target_zeros)\n",
        "        Z_tilde_loss = self.criterion_gen(Z_tilde_confidence, self.target_zeros)\n",
        "\n",
        "        loss_images = (X_loss + X_hat_loss + X_tilde_loss) / 4\n",
        "        loss_latent = (Z_loss + Z_hat_loss + Z_tilde_loss) / 4\n",
        "        loss = loss_images + loss_latent\n",
        "\n",
        "        loss.backward()\n",
        "        self.optim_di.step()\n",
        "        self.optim_dl.step()\n",
        "\n",
        "        return loss_images.item(), loss_latent.item()\n",
        "\n",
        "    def train_epoch(self, print_frequency=1, max_steps=0):\n",
        "        \"\"\"Train both networks for one epoch and return the losses.\n",
        "\n",
        "        Args:\n",
        "            print_frequency (int): print stats every `print_frequency` steps.\n",
        "            max_steps (int): End epoch after `max_steps` steps, or set to 0\n",
        "                             to do the full epoch.\n",
        "        \"\"\"\n",
        "        ldx, ldz, lgx, lgz, lrx, lrz = 0, 0, 0, 0, 0, 0\n",
        "        eps = 1e-9\n",
        "        for batch, (real_samples, _) in enumerate(self.dataloader):\n",
        "            real_samples = real_samples.to(self.device)\n",
        "            ldx_, ldz_ = self.train_step_discriminators(real_samples)\n",
        "            ldx += ldx_\n",
        "            ldz += ldz_\n",
        "            lgx_, lgz_, lrx_, lrz_ = self.train_step_generators(real_samples)\n",
        "            lgx += lgx_\n",
        "            lgz += lgz_\n",
        "            lrx += lrx_\n",
        "            lrz += lrz_\n",
        "            if print_frequency and (batch+1) % print_frequency == 0:\n",
        "                print(f\"{batch+1}/{len(self.dataloader)}:\"\n",
        "                      f\" G={lgx / (eps + (batch+1) * ALPHA_DISCRIMINATE_IMAGE):.3f},\"\n",
        "                      f\" E={lgz / (eps + (batch+1) * ALPHA_DISCRIMINATE_LATENT):.3f},\"\n",
        "                      f\" Dx={ldx / (eps + (batch+1)):.3f},\"\n",
        "                      f\" Dz={ldz / (eps + (batch+1)):.3f}\",\n",
        "                      f\" Rx={lrx / (eps + (batch+1) * ALPHA_RECONSTRUCT_IMAGE):.3f}\",\n",
        "                      f\" Rz={lrz / (eps + (batch+1) * ALPHA_RECONSTRUCT_LATENT):.3f}\",\n",
        "                      end='\\r',\n",
        "                      flush=True)\n",
        "            if max_steps and batch == max_steps:\n",
        "                break\n",
        "        if print_frequency:\n",
        "            print()\n",
        "        lgx /= batch\n",
        "        lgz /= batch\n",
        "        ldx /= batch\n",
        "        ldz /= batch\n",
        "        lrx /= batch\n",
        "        lrz /= batch\n",
        "        return lgx, lgz, ldx, ldz, lrx, lrz\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9tG4NJeNPMm"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqTTo4gQNSIo",
        "outputId": "2a844b0b-896b-4ecb-9e9a-52cc894f2003"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/150 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1; Elapsed time = 00:00:00s\n",
            "Error processing image /content/pokemons/sprites-master/sprites/pokemon/10186.png: cannot identify image file '/content/pokemons/sprites-master/sprites/pokemon/10186.png'\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|          | 1/150 [00:36<1:29:41, 36.12s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2; Elapsed time = 00:00:36s\n",
            "Error processing image /content/pokemons/sprites-master/sprites/pokemon/10186.png: cannot identify image file '/content/pokemons/sprites-master/sprites/pokemon/10186.png'\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|▏         | 2/150 [01:10<1:26:09, 34.93s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3; Elapsed time = 00:01:10s\n",
            "Error processing image /content/pokemons/sprites-master/sprites/pokemon/10186.png: cannot identify image file '/content/pokemons/sprites-master/sprites/pokemon/10186.png'\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 3/150 [01:43<1:24:01, 34.29s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4; Elapsed time = 00:01:43s\n",
            "Error processing image /content/pokemons/sprites-master/sprites/pokemon/10186.png: cannot identify image file '/content/pokemons/sprites-master/sprites/pokemon/10186.png'\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  3%|▎         | 4/150 [02:17<1:22:53, 34.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5; Elapsed time = 00:02:17s\n",
            "Error processing image /content/pokemons/sprites-master/sprites/pokemon/10186.png: cannot identify image file '/content/pokemons/sprites-master/sprites/pokemon/10186.png'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 5/150 [02:51<1:21:55, 33.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6; Elapsed time = 00:02:51s\n",
            "Error processing image /content/pokemons/sprites-master/sprites/pokemon/10186.png: cannot identify image file '/content/pokemons/sprites-master/sprites/pokemon/10186.png'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision as tv\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "LATENT_DIM = 16\n",
        "EPOCHS = 150 # at least 2000 if possible :)\n",
        "VAL_EVERY = 30\n",
        "DATASET_DIR = \"/content/pokemons/sprites-master/sprites/pokemon\"\n",
        "\n",
        "def save_images(GAN, vec, filename):\n",
        "    images = GAN.generate_samples(vec)\n",
        "    ims = tv.utils.make_grid(images[:36], normalize=True, nrow=6,)\n",
        "    ims = ims.numpy().transpose((1,2,0))\n",
        "    ims = np.array(ims*255, dtype=np.uint8)\n",
        "    image = Image.fromarray(ims)\n",
        "    image.save(filename)\n",
        "\n",
        "\n",
        "def main():\n",
        "    os.makedirs(\"results/generated\", exist_ok=True)\n",
        "    os.makedirs(\"results/reconstructed\", exist_ok=True)\n",
        "    os.makedirs(\"results/checkpoints\", exist_ok=True)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    transform = tv.transforms.Compose([\n",
        "            tv.transforms.Resize((96, 96)),\n",
        "            tv.transforms.RandomAffine(0, translate=(5/96, 5/96)),\n",
        "            tv.transforms.ColorJitter(hue=0.5),\n",
        "            tv.transforms.RandomHorizontalFlip(p=0.5),\n",
        "            tv.transforms.ToTensor(),\n",
        "            tv.transforms.Normalize((0.5, 0.5, 0.5,), (0.5, 0.5, 0.5,))\n",
        "            ])\n",
        "    dataset = ImageFolder(\n",
        "            root=DATASET_DIR,\n",
        "            transform=transform\n",
        "            )\n",
        "    dataloader = DataLoader(dataset,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=True,\n",
        "            num_workers=8,\n",
        "            drop_last=True\n",
        "            )\n",
        "    X = iter(dataloader)\n",
        "    test_ims1, _ = next(X)\n",
        "    test_ims2, _ = next(X)\n",
        "    test_ims = torch.cat((test_ims1, test_ims2), 0)\n",
        "    test_ims_show = tv.utils.make_grid(test_ims[:36], normalize=True, nrow=6,)\n",
        "    test_ims_show = test_ims_show.numpy().transpose((1,2,0))\n",
        "    test_ims_show = np.array(test_ims_show*255, dtype=np.uint8)\n",
        "    image = Image.fromarray(test_ims_show)\n",
        "    image.save(\"results/reconstructed/test_images.png\")\n",
        "\n",
        "    noise_fn = lambda x: torch.randn((x, LATENT_DIM), device=device)\n",
        "    test_noise = noise_fn(36)\n",
        "    gan = AEGAN(\n",
        "        LATENT_DIM,\n",
        "        noise_fn,\n",
        "        dataloader,\n",
        "        device=device,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        )\n",
        "    start = time.time()\n",
        "    for i in tqdm(range(EPOCHS)):\n",
        "        while True:\n",
        "            try:\n",
        "                with open(\"pause.json\") as f:\n",
        "                    pause = json.load(f)\n",
        "                if pause['pause'] == 0:\n",
        "                    break\n",
        "                print(f\"Pausing for {pause['pause']} seconds\")\n",
        "                time.sleep(pause[\"pause\"])\n",
        "            except (KeyError, json.decoder.JSONDecodeError, FileNotFoundError):\n",
        "                break\n",
        "        elapsed = int(time.time() - start)\n",
        "        elapsed = f\"{elapsed // 3600:02d}:{(elapsed % 3600) // 60:02d}:{elapsed % 60:02d}\"\n",
        "        print(f\"Epoch {i+1}; Elapsed time = {elapsed}s\")\n",
        "        gan.train_epoch(max_steps=100)\n",
        "        if (i + 1) % VAL_EVERY == 0:\n",
        "            torch.save(\n",
        "                gan.generator.state_dict(),\n",
        "                os.path.join(\"results\", \"checkpoints\", f\"gen.{i:05d}.pt\"))\n",
        "        save_images(gan, test_noise,\n",
        "            os.path.join(\"results\", \"generated\", f\"gen.{i:04d}.png\"))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            reconstructed = gan.generator(gan.encoder(test_ims.cuda())).cpu()\n",
        "        reconstructed = tv.utils.make_grid(reconstructed[:36], normalize=True, nrow=6,)\n",
        "        reconstructed = reconstructed.numpy().transpose((1,2,0))\n",
        "        reconstructed = np.array(reconstructed*255, dtype=np.uint8)\n",
        "        reconstructed = Image.fromarray(reconstructed)\n",
        "        reconstructed.save(os.path.join(\"results\", \"reconstructed\", f\"gen.{i:04d}.png\"))\n",
        "\n",
        "    images = gan.generate_samples()\n",
        "    ims = tv.utils.make_grid(images, normalize=True)\n",
        "    plt.imshow(ims.numpy().transpose((1,2,0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8WT3YCdENLsx"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}